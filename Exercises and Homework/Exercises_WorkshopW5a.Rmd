---
title: 'Exercises for Workshop W5a: RNA-seq I Analysis'
author: "Arjun Bhattacharya"
date: "11/16/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prior to class

1.  Make sure you have access to `hoffman2`. You can do this easily here: <https://www.hoffman2.idre.ucla.edu/Accounts/Requesting-an-account.html>

2.  Once you have an account, we need a few software packages that are not available on `hoffman2`. I'll provide step-by-step instructions.

### Installing FastQC

In your home directory, run the following command to download and unpack the FastQC software:

```{bash, eval=F}
wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.9.zip --no-check-certificate
unzip fastqc_v0.11.9.zip
```

To check if FastQC installed properly:

```{bash, eval=F}
~/FastQC/fastqc --help
```

Next, we have to download and install trimmomatic:

```{bash, eval=F}
wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.39.zip
unzip Trimmomatic-0.39.zip
```

To check if trimmomatic install properly, we'll first have to call an interactive shell and then load up `Java`:

```{bash, eval=F}
qrsh -l h_rt=0:30:00,h_data=5G
module load java
java -jar Trimmomatic-0.39/trimmomatic-0.39.jar
```

Lastly, we have to install Salmon:

```{bash, eval=F}
wget https://github.com/COMBINE-lab/salmon/releases/download/v1.5.1/salmon-1.5.1_linux_x86_64.tar.gz
tar xzvf salmon-1.5.1_linux_x86_64.tar.gz
```

To check if Salmon installed properly:

```{bash, eval=F}
salmon-1.5.1_linux_x86_64/bin/salmon
```

## EXERCISE 1

### Exercise 1A: Logging in (connecting) to `hoffman2`

To log into the cluster, open up a terminal. In Windows, you can use the [PuTTY app](https://www.putty.org/) (or the `cmd` app itself usually works). In Mac or Linux, you can just use the `terminal` app. Once you've opened up your terminal, enter the follow command to connect to `hoffman2`:

```{bash, eval=F}
ssh your_username@hoffman2.idre.ucla.edu
```

**Make sure you put your own username here!**

Important: Every time you connect to the `hoffman2` cluster, you will initially have a session on the login node (login server). Because this computer is shared among all users and has limited computing resources, it's **bad practice** to run even mild computations on this node. The first thing you should do after logging into the cluster is to ask for an interactive session on a worker node, by running the command:

```{bash, eval=F}
qrsh -l h_rt=2:00:00,h_data=8G
```

This opens up a 2 hour session with up to 8 GB of memory. After a few seconds (hopefully, fingers crossed...), you should have been given an interactive session on a worker node. You can tell that you have moved by looking at the server name in your command prompt: the name of `hoffman2` login node is `login2`, while worker nodes have names like `n2190` or `n2236`.

### Exercise 1B: Obtaining the workshop data

Once you are connected to the cluster and logged into a worker node, make a copy of the workshop data in your `home` directory. Let's practice grabbing files and folders from Github. Run the following code to clone the Github repository for this workshop into your `home` directory:

```{bash, eval=F}
git clone https://github.com/bhattacharya-a-bt/QCB-W5a-Nov2021.git
```

-   Exercise: How many folders are in this repository? What commands did you use?

The example data for this workshop is way too big to host on Github. I've kept it in my `SCRATCH` directory. Let's delete the Github clone and recopy:

```{bash, eval=F}
rm -rf QCB-W5a-Nov2021/
cp -r /u/scratch/a/abtbhatt/QCB-W5a-Nov2021/ /u/home/a/abtbhatt/
```

This data is taken from a recent [preprint](https://www.medrxiv.org/content/10.1101/2021.04.12.21255170v3). The original data, fully compressed, is around 20 GB. Here, we're only looking at a subset of the data:

-   Only two samples

-   Only around 1-1.5 million reads per sample

## EXERCISE 2: FASTQ files and quality control

### Exercise 2A: Working with FASTQ files

Let's look at these two files: `KD1.fastq.gz` and `SC1.fastq.gz` , which comprise the sequence reads for `KD` and `SC` in FASTQ format.

*Note: This file has been compressed with `GZIP`, so you may need to decompress it using `gzip` or `zcat` to access the FASTQ text contained. Some programs (such as the `less` text viewer, and many bioinformatic tools) know how to deal with gzipped files.*

-   How many lines does each file have? How many reads do we have for each sample?

-   How long is the first read for `KD1.fastq.gz` ?

-   What is the PHRED-scaled confidence for the first base call of the first read?

### Exercise 2B: Quality control with FastQC

FastQC is a program that parses FASTQ read files and outputs a number of human-readable summary statistics and plots. Let's use it to check the quality of the sequence data for our two samples, `KD` and `SC`. The `hoffman2` cluster does not provide the FastQC program, but you should have already installed it to your home directory. Because we are using a personal copy of the program rather than a system-wide installation, to run it we have the provide the full path to the main file of the program:

```{bash, eval=F}
~/FastQC/fastqc --help
```

Note: `--help` or `-h` are standard options, which many programs recognize as a request for information about the software.

So now, we can run FastQC on the reads of `KD1.fastq.gz` like this:

```{bash, eval=F}
~/FastQC/fastqc ~/QCB-W5a-Nov2021/Data/KD1.fastq.gz
```

-   Run FastQC on both sequence files. Wait for the program to complete (this should only take about a minute).

-   How many output files did each run create? (Note: You can list the files in a directory in the order in which they were create using `ls -ltr`.)

### Exercise 2C: Transferring files from/to the cluster

The easiest way to transfer files is to use an SFTP client with a graphical interface, like [WinSCP](https://winscp.net/eng/download.php) or [FileZilla](https://filezilla-project.org/download.php?type=server) for Windows or [FileZilla](https://filezilla-project.org/download.php?type=server) or [Cyberduck](https://cyberduck.io/) for Mac or Linux.

Alternatively, files can be transfered using the command-line utilities `SCP` and `RSYNC`. Yet larger datasets may be transfered from/to `hoffman2` using the grid-computing toolkit [Globus](https://www.hoffman2.idre.ucla.edu/Using-H2/Data-transfer.html).

-   Download the output files from FastQC to your laptop. They are HTML files, so we can open them with a web browser.

-   Are there any obvious quality differences between the two samples?

## EXERCISE 3: Filtering low-quality reads

The first step in a sequence data analysis is usually to remove the subset of the data that has insufficient quality -- keeping unreliable reads and base calls can introduce unnecessary noise in the analysis.

To do so, we will use the program `trimmomatic`. Like for FastQC above, the `hoffman2` cluster doesn't provide `trimmomatic`, but we did install this to our home directory.

The `.jar` extension in the program name indicates that this is a Java program and should be run not directly by through the Java program. First, we have to load the Java module with `module load java`.

Unlike most programs, `trimmomatic` does not have much built-in documentation -- in particular, the `-h`/`--help` documentation is only minimaly useful. We can instead refer to the `Trimmomatic` [website](http://www.usadellab.org/cms/?page=trimmomatic) which gives this example for single-end data:

```{bash, eval=F}
java -jar trimmomatic-0.35.jar SE -phred33 input.fastq.gz   output.fastq.gz ILLUMINACLIP:TruSeq3-SE:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36
```

Let's do the following:

-   Copy-paste the above command in a text editor, like Notepad or TextWrangler.

-   According to the documentation on the `trimmomatic` home page, what are the different parts of the command doing?

-   Modify the command so that:

    -   it uses the correct path to the program in your home directory

    -   it uses `KD1.fastq.gz` are its input file

    -   the output file is named after the input file but is distinguishable from the input file

    -   it asks for minimum leading and trailing base qualities of 20

    -   it asks for a minimum final read length of 60 basepairs (or 80% of the raw read length)

    -   at the end of the command line, add the option `-threads 1`. This is to use a single thread/processor, as the default is to use several.

-   Run the command, wait for it to finish (shouldn't take that long), and then make sure the output file exists and is not empty. How many reads are in the output file? Does this reflect what `trimmomatic` outputs onto the terminal?

-   Modify the command to process the other sample and check its output file.

## EXERCISE 4: Aligning reads to the reference genome

Here, we will align our RNA-seq Illumina reads to the reference human genome using the program `STAR`.

### Exercise 4A: Creating a STAR genome database

Read alignment programs typically require the construction of a genome sequence database as a preliminary step before they are able to perform read alignments. This is necessary because aligning directly to a genome sequence in FASTA format is not computationally efficient, but only needs to be run once.

To create a genome database against which to map reads, `STAR` needs the genome sequence (FASTA file) and the gene/transcript/exon annotations (GTF file).

Note: Because of time and memory constraints during the workshop, we will not use the full genome, but actually only Chromosome 18.

-   `hoffman2` provides `STAR` as a module. Load the `STAR` module with the command `module load star`.

-   Inside your `QCB-W5a-Nov2021/` directory, create a `star_database/` subdirectory and `cd` inside this directory.

-   Create a `STAR` database using the following commands:

```{bash, eval=F}
fasta_file=/u/home/a/abtbhatt/QCB-W5a-Nov2021/Data/GRCh38_chr18.fa

STAR \
--runMode genomeGenerate \
--genomeFastaFiles $fasta_file \
--genomeSAindexNbases 12 \
--genomeDir ./
```

This command should take a bit of time to complete too.

While we wait: what are these commands doing?

-   Once `STAR` has completed, check for errors in the terminal and in the `STAR` log file, and for existing non-empty output files.

### Exercise 4B: Aligning reads to the genome

-   To align the cleaned reads of `KD`, we can run the following commands:

    ```{bash, eval=F}
    cd ~/QCB-W5a-Nov2021/Data/
    star_db_dir=/u/home/a/abtbhatt/QCB-W5a-Nov2021/star_database/
    sample=KD1

    STAR \
    --genomeDir $star_db_dir \
    --readFilesIn $sample.fastq.gz \
    --outFileNamePrefix ./$sample. \
    --outSAMtype BAM SortedByCoordinate \
    --outFilterMismatchNmax 5 \
    --outFilterMultimapNmax 1 \
    --readFilesCommand zcat 
    ```

After the program completes, check for errors in the terminal and in the `STAR` log file (especially toward the end), and for existing non-empty output files.

While the program runs, try to understand the above command. You can find the description of each `STAR` option we have used in the help (`STAR --help`). You can also star installing `IGV` on your laptop (see next exercise).

-   Review the alignment statistics in the `Log.final.out` output file. What proportion of reads mapped unambiguously to the genome database?

### Exercise 4C: Visualizing read alignments

It is sometimes useful to visualize read alignments directly. It should give you a feel for the contents of BAM files, and it is also useful to gain an more intuitive, human understanding of how the raw data behaves at a gene of particular biological interest -- command line tools can fail to appreciate complex patterns such as alignment artifacts, or issues related to genetic diversity or splicing.

One popular visualization tool is the Integrated Genomics Viewer (`IGV`); let's open our BAM files in `IGV`.

-   Install `IGV` on your laptop for [here](https://software.broadinstitute.org/software/igv/download).

-   On the cluster, using the `samtools` program, create indexes for each of your two read alignment `BAM` files. You can use the `samtools index` command (but first load `samtools`). This will create a `BAM` index (`BAI`) file for each `BAM` file.

-   Download your two `BAM` files and their two `BAI` files to your laptop.

-   Start IGV, and open the human genome. Go to File \> Load from file and open your two `BAM` files.

-   Navigate to the *NOL4* gene using the search box. How do the two samples differ?

## EXERCISE 5: Quantifying per-gene expression

### Exercise 5A: Installing HTSeq

To quantify expression at the gene level, we will use the `HTSeq` program. It is not provided by `hoffman2`, nor in the workshop's directory. However, you can easily install it yourself.

First, load the Python3 module with the command `module load python/3.9.6`.

You can then install `HTSeq` as a Python3 package:

```{bash, eval=F}
python3 -m pip install --user HTSeq
```

This will install `HTSeq` at `~/.local/bin/htseq-count`.

### Exercise 5B: Tallying per-gene read counts

Use `HTSeq` to count the number of reads mapping to each gene.

```{bash, eval=F}
cd ~/QCB-W5a-Nov2021/Data
sample=KD1
bam_file=$sample.Aligned.sortedByCoord.out.bam
gtf_file=gencode.v38_chr18.gtf
~/.local/bin/htseq-count \
--idattr=gene_id \
--type=exon --mode=union \
--format=bam \
--stranded=yes \
$bam_file \
$gtf_file \
> $sample.pergene_counts
```

This should take about 1 minute. Make sure you understand what the above command is doing. The `htseq-count` options are described in the `HTSeq` help (available with `htseq-count --help`).

### Exercise 5C: Merging all samples' counts into one big table

`HTSeq` generates a separate table of per-gene read counts for each sample. For statistical analyses, it is more practical to merge all these tables into one large table comprising the counts at all genes and for all samples.

There are many alternative ways to generate such a table; one such way is using the `R` package `edgeR`.

-   On the cluster, load `R`: `module load R/4.1.0-BIO`.

-   Start an `R` session by entering `R`.

-   `edgeR` is installed already.

-   In `R`, load, merge, and save the global counts table using the following `R` commands.

```{r,eval=F}
library(edgeR)
samples <- c('KD1', 'SC1')
files <- paste0(samples,'.pergene_counts')
counts <- readDGE(files, labels=samples, header=FALSE)
write.csv(counts$counts, 'counts.csv')
```

## EXERCISE 6: Salmon and tximeta

We first need to grab a reference transcriptome that we can "pseudo-align" our reads to. We're using the `hg38` reference transcriptome from ensembl. In command line:

```{bash, eval=F}
cd ~/QCB-W5a-Nov2021/Data
wget ftp://ftp.ensembl.org/pub/release-95/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz
gunzip Homo_sapiens.GRCh38.cdna.all.fa.gz
```

### Exercise 6A: Indexing the transcriptome

Next, we're going to build an **index** on our transcriptome. The index is a structure that salmon uses to [quasi-map](%5Bhttp://bioinformatics.oxfordjournals.org/content/32/12/i192.abstract)](<http://bioinformatics.oxfordjournals.org/content/32/12/i192.abstract>)) RNA-seq reads during quantification. The index need only be constructed once per transcriptome, and it can then be reused to quantify many experiments. We use the \*index\* command of salmon to build our index:

```{bash,eval=F}
salmon=/u/home/a/abtbhatt/salmon-1.5.1_linux_x86_64/bin/salmon
$salmon index -t Homo_sapiens.GRCh38.cdna.all.fa -i human_hg19_index -k 21
```

This will take a pretty long time - don't worry if you don't finish!

### Exercise 6B: Quantifying reads

Now, we can create a for loop to iterate over all FASTQ samples. Inside the loop we will create a variable that stores the prefix we will use for naming output files, then we run Salmon. All of this is explained in detail in the [salmon](https://salmon.readthedocs.io/en/latest/salmon.html)) tutorial.

```{bash,eval=F}
for fq in *.fastq.gz

do

# create a prefix
base=`basename $fq .fastq.gz`

# run salmon
$salmon quant -i human_hg19_index \
 -l A \
 -r $fq \
 -p 2 \
 -o $base.salmon \
 --seqBias \
 --useVBOpt \
 --validateMappings

done
```

In the folder, you'll see a corresponding folder for each sample, which contains a file named `quant.sf`. This contains the gene expression data.

### Exercise 6C: Importing the data

Now, we'll be using `tximeta` to import this data. A full-blown tutorial is available [here](https://www.bioconductor.org/packages/devel/bioc/vignettes/tximeta/inst/doc/tximeta.html). First, let's create a column data data frame that we'll use to read in our data. Open up `R` again.

We'll most likely have to install the package first from `bioconductor`.

```{r,eval=F}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("tximeta")

require(tximeta)
coldata <- data.frame(files = c('KD1.salmon/quant.sf',
                                'SC1.salmon/quant.sf'), 
                      names=c('KD1',
                              'SC1'), 
                      condition=c(rep('Knockdown',1),
                                  rep('Scramble',1)), 
                      stringsAsFactors=FALSE)
coldata$condition = factor(coldata$condition,
                           levels = c('Scramble',
                                      'Knockdown'))

se <- tximeta(coldata)
library(SummarizedExperiment)
gse <- summarizeToGene(se, countsFromAbundance="lengthScaledTPM")
```

Now, `gse` contains the gene expression data in a `SummarizedExperiment`. You can extract the counts as a matrix using `assays(gse)$counts`.
